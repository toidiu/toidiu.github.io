+++
title = "HA of DS recommendation"
date = 2019-09-11

[extra]
company = "iHeartRadio"
+++

## list
- DS HA
- migrate jenkins from box to kube
- jvm debugging
- max connection failure for > 5 services (websocket). checking code
- skynet - QA code to injest charles data and run tests
- fastly migration from blacklisting(dont cache this) to whitelisting(cache this)
- production outage - tracking list of services and restarting them
- max ok http connections, adding configuration to detect mismatch vs auto set
- decision under not enough information: pagerduty was going of due to a long rabbit queue. it was very late none of the other parties were responding
  so i had to make a decision on how to proceed.
- challenged org: recs team did not have enough resources but was expected to produce results, that were based on hunches.
  I worked and got the right parties involved to get more resources to the team.
- customer was charged for subscription but didnt get features. on call issue meant doing right by customer
  and working with cus-support and communicating technical findings to customer support and client

- starting and leading the rust meetup group
- commiting to rust lang issue and following thru on it (7 months)
- taking up postgres-mapper and appending functionality to it (fixing bugs)

 - Worked closely with Data Science/Design/iOS to help launch music recommendaRon service and make it more resilient.
     - migrating old recs api to new micro service logic
     - HA for data published by DS
     - realizing fastly was caching recs
       - change from blacklist to whitelist and rolling it out in production

 - debugging and optimizing Mongo query performance across different services
 - write unit and mongo embed tests across the many services
 - I interface with kubernetes on a daily basis. k8s is quite complex so many of the debugging tasks involve creating and refining the mental picture of how it actually works. This is then used to debug and further improve upon service performance.

Access and expose data recommendations generated by Data Science (DS).

=== Problem statement:

Data Science (DS) ran a nightly job to generate music recommendations for users. The dataset would live in DynamoDB and the old DS workflow was to rewrite the same dynamo table with new dataset each night. This was a high risk operation for the my team (APIs). Additionally it caused some outages due to accidental schema changes.

I was in charge of creating a HA and resilient workflow.

=== Design:

 Treat data as immutable
     Enforce that DS publish the dataset to a new table each night
     This gives the benefit of 'rollback' if a new dataset was broken
 Given multiple datasets (a,b,c); DS can 'point' to latest dataset
     DS can maintain a few versions of the data. "Version Table"
     DS can run tests on new datasets to confirm schema compatibility
     rollbacks are as easy as pointing to an older known dataset
 Maintain a log of actions (publish new dataset, test pass/fail, point to new dataset)

=== Tasks performed:

 Work closely with DS to come up with the system design.
 Added a Jenkins tests which could be triggered by DS
     used to verify that data schema would not be a breaking change
     extensible model for other types of tests
     potential to track failing/passing metrics
 Poll based mechanism within API code to look at "Version Table" and start using the latest dataset.
     A poll interval of 5 min was used since 'older' data would still produce good enough data.
     Log when a new dataset was detected (help correlate errors with new dataset) 


