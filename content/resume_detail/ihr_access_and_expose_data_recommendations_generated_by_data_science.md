+++
title = "Access and expose data recommendations generated by Data Science"
date = 2019-09-11

[extra]
company = "iHeartRadio"
lp = ["highest standard"]
+++

### S
Data Science (DS) ran a nightly job to generate music recommendations for users. The dataset would live in DynamoDB and the old DS workflow was to rewrite the same dynamo table with new dataset each night. This was a high risk operation for the my team (APIs). Additionally it caused some outages due to accidental schema changes.

I was in charge of creating a HA and resilient workflow.

=== Design:

 Treat data as immutable
     Enforce that DS publish the dataset to a new table each night
     This gives the benefit of 'rollback' if a new dataset was broken
 Given multiple datasets (a,b,c); DS can 'point' to latest dataset
     DS can maintain a few versions of the data. "Version Table"
     DS can run tests on new datasets to confirm schema compatibility
     rollbacks are as easy as pointing to an older known dataset
 Maintain a log of actions (publish new dataset, test pass/fail, point to new dataset)

=== Tasks performed:

 Work closely with DS to come up with the system design.
 Added a Jenkins tests which could be triggered by DS
     used to verify that data schema would not be a breaking change
     extensible model for other types of tests
     potential to track failing/passing metrics
 Poll based mechanism within API code to look at "Version Table" and start using the latest dataset.
     A poll interval of 5 min was used since 'older' data would still produce good enough data.
     Log when a new dataset was detected (help correlate errors with new dataset) 


